{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "859b0077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5dae768",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"./archive/\"\n",
    "trainDF = []\n",
    "testDF = []\n",
    "\n",
    "features = ['DOY', 'DOW', 'CRS_DEP_TIME', 'DISTANCE']\n",
    "target = ['ARR_DELAY']\n",
    "\n",
    "for (root, dirs, filenames) in os.walk(\"./archive/airline delay analysis\"):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(root, filename)\n",
    "            \n",
    "            name, extension = os.path.splitext(filename)\n",
    "            \n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            df['FL_DATE'] = pd.to_datetime(df['FL_DATE'], format='%Y-%m-%d')\n",
    "            df['DOY']=df['FL_DATE'].dt.dayofyear\n",
    "            df['DOW']=df['FL_DATE'].dt.dayofweek\n",
    "            #df['YEAR'] = df['FL_DATE'].dt.year\n",
    "            #df['MONTH'] = df['FL_DATE'].dt.month\n",
    "            #df['DAY'] = df['FL_DATE'].dt.day\n",
    "\n",
    "            df = df[features + target]\n",
    "            if name == '2018':\n",
    "                testDF = df\n",
    "            elif name == '2009': \n",
    "                trainDF.append(df)\n",
    "\n",
    "trainDF = list(trainDF)\n",
    "trainDF = pd.concat(trainDF, ignore_index=True)\n",
    "\n",
    "columnToCheck = 'ARR_DELAY'\n",
    "condition = trainDF[columnToCheck].abs() > 60\n",
    "trainDF = trainDF[~condition]\n",
    "condition = testDF[columnToCheck].abs() > 60\n",
    "testDF = testDF[~condition]\n",
    "\n",
    "trainDF = trainDF.dropna()\n",
    "testDF = testDF.dropna()\n",
    "\n",
    "train_x = trainDF[features]\n",
    "train_y = trainDF[target]\n",
    "\n",
    "test_x = testDF[features]\n",
    "test_y = testDF[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64f385b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "class SimpleRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleRegressor, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 32)\n",
    "        self.fc2 = nn.Linear(16,64)\n",
    "        self.fc3 = nn.Linear(32, 8)\n",
    "        self.fc4 = nn.Linear(8, 1)\n",
    "        #self.dropout = nn.Dropout(0.4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        #x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        #x = torch.clamp(x, -66, 66)\n",
    "        #x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs):\n",
    "    #clip_value = 1.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.view(-1)\n",
    "            labels = labels.view(-1)\n",
    "            #print(outputs, labels)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        #scheduler.step()\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "def accuracy(predictions, targets, threshold=6):\n",
    "    absolute_errors = torch.abs(predictions - targets)\n",
    "    correct_predictions = (absolute_errors <= threshold).float()\n",
    "    accuracy = correct_predictions.mean().item()\n",
    "    return accuracy\n",
    "\n",
    "def evaluate_model(model, test_features, test_labels, criterion, threshold=6):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    total_accuracy = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(test_features)\n",
    "        print(outputs)\n",
    "        labels = test_labels.view(-1)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        outputs = outputs.view(-1)\n",
    "        acc = accuracy(outputs, labels, threshold=threshold)\n",
    "        total_accuracy += acc\n",
    "\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    mean_loss = total_loss\n",
    "    mean_accuracy = total_accuracy\n",
    "    print(f\"Loss: {mean_loss}, Accuracy: {mean_accuracy}\")\n",
    "\n",
    "    return mean_loss, mean_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7450b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1/1, Loss: 285.1103866792119\n",
      "tensor([[-1.2188],\n",
      "        [-1.2188],\n",
      "        [-1.2188],\n",
      "        ...,\n",
      "        [-1.2188],\n",
      "        [-1.2188],\n",
      "        [-1.2188]], device='cuda:0')\n",
      "Loss: 283.6912536621094, Accuracy: 0.328779399394989\n",
      "Fold 1 - Test Loss: 283.6912536621094, Accuracy: 0.328779399394989\n",
      "Fold 2\n"
     ]
    }
   ],
   "source": [
    "# PyTorch 텐서로 변환\n",
    "train_x_tensor = torch.tensor(train_x.values, dtype=torch.float32)\n",
    "train_y_tensor = torch.tensor(train_y.values, dtype=torch.float32)\n",
    "\n",
    "# PyTorch 텐서로 변환\n",
    "test_x_tensor = torch.tensor(test_x.values, dtype=torch.float32)\n",
    "test_y_tensor = torch.tensor(test_y.values, dtype=torch.float32)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_x_tensor = train_x_tensor.to(device)\n",
    "train_y_tensor = train_y_tensor.to(device)\n",
    "\n",
    "test_x_tensor = test_x_tensor.to(device)\n",
    "test_y_tensor = test_y_tensor.to(device)\n",
    "\n",
    "model = SimpleRegressor()\n",
    "model = model.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fold_losses = []\n",
    "fold_accuracies = []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(train_x_tensor)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    \n",
    "    fold_train_x, fold_test_x = train_x_tensor[train_index], train_x_tensor[test_index]\n",
    "    fold_train_y, fold_test_y = train_y_tensor[train_index], train_y_tensor[test_index]\n",
    "\n",
    "    # Move fold data to PyTorch tensors and then to the device\n",
    "    fold_train_x = fold_train_x.to(device)\n",
    "    fold_train_y = fold_train_y.to(device)\n",
    "    fold_test_x = fold_test_x.to(device)\n",
    "    fold_test_y = fold_test_y.to(device)\n",
    "\n",
    "    # Create DataLoader for training and testing data\n",
    "    train_dataset = TensorDataset(fold_train_x, fold_train_y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    test_dataset = TensorDataset(fold_test_x, fold_test_y)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # Create an instance of the AirlineDelayPredictionModel class\n",
    "    model = SimpleRegressor().to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    #scheduler = StepLR(optimizer, step_size=1, gamma=0.2)\n",
    "    # Train the model\n",
    "    train_model(model, train_loader, criterion, optimizer, epochs=1)\n",
    "    \n",
    "    # Evaluate the model on the test data\n",
    "    fold_loss, fold_accuracy = evaluate_model(model, fold_test_x, fold_test_y, criterion, threshold=6)\n",
    "    print(f\"Fold {fold + 1} - Test Loss: {fold_loss}, Accuracy: {fold_accuracy}\")\n",
    "\n",
    "    fold_losses.append(fold_loss)\n",
    "    fold_accuracies.append(fold_accuracy)\n",
    "\n",
    "average_loss = sum(fold_losses) / len(fold_losses)\n",
    "average_accuracy = sum(fold_accuracies) / len(fold_accuracies)\n",
    "\n",
    "print(f\"Overall Performance - Average Test Loss: {average_loss}, Average Accuracy: {average_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1877ed71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0814],\n",
      "        [-1.0814],\n",
      "        [-1.0814],\n",
      "        ...,\n",
      "        [-1.0814],\n",
      "        [-1.0814],\n",
      "        [-1.0814]], device='cuda:0')\n",
      "correct_predictions : tensor([0., 0., 0.,  ..., 0., 0., 1.], device='cuda:0')\n",
      "0.2766715884208679\n",
      "Loss: 323.1086730957031, Accuracy: 0.2766715884208679\n",
      "Loss: 323.1086730957031, accuracy: <function accuracy at 0x00000145D12FCCA0>\n"
     ]
    }
   ],
   "source": [
    "loss, acciracy = evaluate_model(model, test_x_tensor, test_y_tensor, criterion, threshold=6)\n",
    "print(f'Loss: {loss}, accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f28e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    \n",
    "    outputs = model(train_x_tensor)\n",
    "    loss = criterion(outputs, train_y_tensor)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation loss\n",
    "    val_outputs = model(test_x_tensor)\n",
    "    val_loss = criterion(val_outputs, test_y_tensor)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Validation Loss: {val_loss.item():.4f}')\n",
    "\n",
    "num_epochs = 100\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    \n",
    "    outputs = model(train_x_tensor)\n",
    "    loss = criterion(outputs, train_y_tensor)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation loss\n",
    "    val_outputs = model(test_x_tensor)\n",
    "    val_loss = criterion(val_outputs, test_y_tensor)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Validation Loss: {val_loss.item():.4f}')\n",
    "\n",
    "# 학습된 모델로 예측\n",
    "\n",
    "loss, acciracy = evaluate_model(model, test_x_tensor, test_y_tensor, criterion, threshold=6)\n",
    "print(f'Loss: {loss}, accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b530c1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tDF['DOY_DOW_pair'] = list(zip(tDF['DOY'], tDF['DOW']))\n",
    "print(tDF)\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "for idx, row in tDF.iterrows():\n",
    "    G.add_node(idx, features=row[['DOY_DOW_pair', 'CRS_DEP_TIME', 'DISTANCE']].values, label=row['ARR_DELAY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f101b06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fold_losses = []\n",
    "fold_accuracies = []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(train_x_tensor)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    \n",
    "    fold_train_x, fold_test_x = train_x_tensor[train_index], train_x_tensor[test_index]\n",
    "    fold_train_y, fold_test_y = train_y_tensor[train_index] / 60, train_y_tensor[test_index] / 60\n",
    "\n",
    "    # Move fold data to PyTorch tensors and then to the device\n",
    "    fold_train_x = fold_train_x.to(device)\n",
    "    fold_train_y = fold_train_y.to(device)\n",
    "    fold_test_x = fold_test_x.to(device)\n",
    "    fold_test_y = fold_test_y.to(device)\n",
    "\n",
    "    # Create DataLoader for training and testing data\n",
    "    train_dataset = TensorDataset(fold_train_x, fold_train_y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    test_dataset = TensorDataset(fold_test_x, fold_test_y)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # Create an instance of the AirlineDelayPredictionModel class\n",
    "    model = SimpleRegressor().to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=0.2)\n",
    "    # Train the model\n",
    "    train_model(model, train_loader, criterion, optimizer, scheduler, epochs=20)\n",
    "    \n",
    "    # Evaluate the model on the test data\n",
    "    fold_loss, fold_accuracy = evaluate_regression_model(model, test_loader, criterion, threshold=0.1)\n",
    "    print(f\"Fold {fold + 1} - Test Loss: {fold_loss}, Accuracy: {fold_accuracy}\")\n",
    "\n",
    "    fold_losses.append(fold_loss)\n",
    "    fold_accuracies.append(fold_accuracy)\n",
    "\n",
    "average_loss = sum(fold_losses) / len(fold_losses)\n",
    "average_accuracy = sum(fold_accuracies) / len(fold_accuracies)\n",
    "\n",
    "print(f\"Overall Performance - Average Test Loss: {average_loss}, Average Accuracy: {average_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45211993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_subgraph_nodes(graph, num_nodes):\n",
    "    return list(graph.nodes)[:num_nodes]\n",
    "\n",
    "def train_subgraph_model(model, subgraph, features, labels, criterion, optimizer, num_epochs=100):\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(features, subgraph)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "#def evaluate_subgraph_model(model, features, adjacency_matrix, labels, criterion):\n",
    "#    with torch.no_grad():\n",
    "#        output = model(features, adjacency_matrix)\n",
    "#        loss = criterion(output, labels)\n",
    "#        print(f'Subgraph Model Loss: {loss.item()}')\n",
    "\n",
    "def accuracy(predictions, targets, threshold=0.1):\n",
    "    absolute_errors = torch.abs(predictions - targets)\n",
    "    correct_predictions = (absolute_errors <= threshold).float()\n",
    "    accuracy = correct_predictions.mean().item()\n",
    "    return accuracy\n",
    "\n",
    "def evaluate_subgraph_model(model, test_features, test_adjacency, test_labels, criterion, threshold=0.1):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    total_accuracy = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(test_features, test_adjacency)\n",
    "        labels = test_labels.view(-1)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # 정확도 계산\n",
    "        acc = accuracy(outputs, labels, threshold=threshold)\n",
    "        total_accuracy += acc\n",
    "\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    mean_loss = total_loss / total_samples\n",
    "    mean_accuracy = total_accuracy / total_samples\n",
    "    print(f\"Subgraph Model Loss: {mean_loss}, Accuracy: {mean_accuracy}\")\n",
    "\n",
    "    return mean_loss, mean_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bc8aaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.gc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, adjacency_matrix):\n",
    "        x = torch.matmul(x, self.gc1.weight.t())\n",
    "        x = torch.matmul(adjacency_matrix, x.t()).t()\n",
    "        x = torch.relu(x)\n",
    "        x = torch.matmul(adjacency_matrix, x.t()).t()\n",
    "        x = self.gc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fe844b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NetworkXNotImplemented",
     "evalue": "not implemented for directed type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNetworkXNotImplemented\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m num_nodes_per_subgraph \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#undirected_G = G.to_undirected()\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#connected_components = list(nx.connected_components(undirected_G))\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m connected_components \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnected_components\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\networkx\\classes\\backends.py:148\u001b[0m, in \u001b[0;36m_dispatch.<locals>.wrapper\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m NetworkXNotImplemented(\n\u001b[0;32m    146\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not implemented by \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplugin_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    147\u001b[0m             )\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32m<class 'networkx.utils.decorators.argmap'> compilation 4:3\u001b[0m, in \u001b[0;36margmap_connected_components_1\u001b[1;34m(G)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbz2\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgzip\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\networkx\\utils\\decorators.py:86\u001b[0m, in \u001b[0;36mnot_implemented_for.<locals>._not_implemented_for\u001b[1;34m(g)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_not_implemented_for\u001b[39m(g):\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (mval \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m mval \u001b[38;5;241m==\u001b[39m g\u001b[38;5;241m.\u001b[39mis_multigraph()) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m     84\u001b[0m         dval \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m dval \u001b[38;5;241m==\u001b[39m g\u001b[38;5;241m.\u001b[39mis_directed()\n\u001b[0;32m     85\u001b[0m     ):\n\u001b[1;32m---> 86\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m nx\u001b[38;5;241m.\u001b[39mNetworkXNotImplemented(errmsg)\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m g\n",
      "\u001b[1;31mNetworkXNotImplemented\u001b[0m: not implemented for directed type"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "input_dim = 3  # features의 차원\n",
    "hidden_dim = 64\n",
    "output_dim = 1  # 예측할 값이 하나이므로 output_dim을 1로 설정\n",
    "subgraph_model = GCN(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.MSELoss()\n",
    "subgraph_optimizer = optim.Adam(subgraph_model.parameters(), lr=0.01)\n",
    "\n",
    "# Subgraph 학습\n",
    "num_subgraph_epochs = 10\n",
    "num_nodes_per_subgraph = 64\n",
    "#undirected_G = G.to_undirected()\n",
    "#connected_components = list(nx.connected_components(undirected_G))\n",
    "connected_components = list(nx.connected_components(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34267e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_nodes, test_nodes = train_test_split(list(G.nodes), test_size=0.2, random_state=42)\n",
    "\n",
    "train_subgraph = G.subgraph(train_nodes)\n",
    "test_subgraph = G.subgraph(test_nodes)\n",
    "\n",
    "# train_subgraph의 노드를 직접 리스트로 변환하지 않고 순회하면서 데이터 추출\n",
    "train_features = torch.FloatTensor([G.nodes[node]['features'] for node in train_subgraph.nodes()])\n",
    "train_labels = torch.FloatTensor([G.nodes[node]['label'] for node in train_subgraph.nodes()])\n",
    "\n",
    "# train_subgraph의 인접 행렬을 생성\n",
    "train_adjacency = torch.Tensor(nx.linalg.graphmatrix.adjacency_matrix(train_subgraph).todense())\n",
    "\n",
    "for subgraph_nodes in connected_components:\n",
    "    # Subgraph에 사용할 일부 노드 선택\n",
    "    subgraph_nodes = select_subgraph_nodes(undirected_G.subgraph(train_subgraph_nodes), num_nodes_per_subgraph)\n",
    "    \n",
    "    # Subgraph에 대한 학습 데이터 생성\n",
    "    subgraph_features = train_features[subgraph_nodes]\n",
    "    subgraph_labels = train_labels[subgraph_nodes]\n",
    "    subgraph_adjacency = train_adjacency[subgraph_nodes][:, subgraph_nodes]\n",
    "    \n",
    "    # Subgraph 모델 학습\n",
    "    train_subgraph_model(subgraph_model, subgraph_adjacency, subgraph_features, subgraph_labels, criterion, subgraph_optimizer, num_epochs=num_subgraph_epochs)\n",
    "    \n",
    "evaluate_subgraph_model(subgraph_model, test_features, test_adjacency, test_labels, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5061b4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "connected_components = list(nx.connected_components(G))\n",
    "\n",
    "for subgraph_nodes in connected_components:\n",
    "    subgraph = G.subgraph(subgraph_nodes)\n",
    "    \n",
    "    subgraph_features = features[subgraph_nodes]\n",
    "    subgraph_labels = labels[subgraph_nodes]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d9b673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff2e19e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd586760",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8ada42",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_matrix = nx.linalg.graphmatrix.adjacency_matrix(subgraph).todense()\n",
    "    \n",
    "feature_matrix = np.array([row['DOY_DOW_pair', 'CRS_DEP_TIME', 'DISTANCE'] for idx, row in tDF.iterrows()])\n",
    "\n",
    "label_binarizer = LabelBinarizer()\n",
    "labels = label_binarizer.fit_transform(tDF['ARR_DELAY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7598098",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "\n",
    "for idx, row in trainDF.iterrows():\n",
    "    G.add_node(idx, features=row[['DOY', 'DOW', 'CRS_DEP_TIME', 'DISTANCE']].values, label=row['ARR_DELAY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961050f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_matrix = nx.linalg.graphmatrix.adjacency_matrix(G).todense()\n",
    "    \n",
    "feature_matrix = np.array([row['DOY', 'DOW', 'CRS_DEP_TIME', 'DISTANCE'] for idx, row in trainDF.iterrows()])\n",
    "\n",
    "label_binarizer = LabelBinarizer()\n",
    "labels = label_binarizer.fit_transform(trainDF['ARR_DELAY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95d3f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AirlineDelayPredictionModel(nn.Module):\n",
    "    def __init__(self, timesteps, features):\n",
    "        super(AirlineDelayPredictionModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=features, out_channels=9, kernel_size=1, stride=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=9, out_channels=15, kernel_size=1, stride=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=15, out_channels=24, kernel_size=1, stride=1)\n",
    "        self.pool = nn.AvgPool1d(kernel_size=2, stride=1)\n",
    "        self.lstm = nn.LSTM(input_size=24, hidden_size=200, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(400, 1)  # Adjusted input size of the linear layer to match LSTM output\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv1(x)\n",
    "        padding = (0, 1)\n",
    "        x = F.pad(x, padding)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.pad(x, padding)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.pad(x, padding)\n",
    "        x = self.pool(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x[:, -1, :])\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(-1)\n",
    "        return x\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, scheduler, epochs):\n",
    "    clip_value = 1.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            labels = labels.view(-1)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "def accuracy(predictions, targets, threshold=0.1):\n",
    "    absolute_errors = torch.abs(predictions - targets)\n",
    "    correct_predictions = (absolute_errors <= threshold).float()\n",
    "    accuracy = correct_predictions.mean().item()\n",
    "    return accuracy\n",
    "\n",
    "def evaluate_regression_model(model, test_loader, criterion, threshold=0.1):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    total_accuracy = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            labels = labels.view(-1)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # 정확도 계산\n",
    "            acc = accuracy(outputs, labels, threshold=threshold)\n",
    "            total_accuracy += acc\n",
    "\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    mean_loss = total_loss / len(test_loader)\n",
    "    mean_accuracy = total_accuracy / len(test_loader)\n",
    "    print(f\"Test Loss: {mean_loss}, Accuracy: {mean_accuracy}\")\n",
    "\n",
    "    return mean_loss, mean_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58fa1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 데이터를 그래프로 변환하는 함수\n",
    "def create_graph_data(df, features, target):\n",
    "    # 데이터 정규화\n",
    "    scaler = StandardScaler()\n",
    "    df[features] = scaler.fit_transform(df[features])\n",
    "\n",
    "    # 그래프 구조 생성\n",
    "    adjacency_matrix = torch.tensor(df.corr().abs().values, dtype=torch.float32)\n",
    "\n",
    "    # 데이터와 인접 행렬 반환\n",
    "    x = torch.tensor(df[features].values, dtype=torch.float32)\n",
    "    y = torch.tensor(df[target].values, dtype=torch.float32)\n",
    "    return x, adjacency_matrix, y\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.gc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, adjacency_matrix):\n",
    "        x = torch.matmul(x, self.gc1.weight.t())  # 가중치 행렬을 전치하여 곱셈\n",
    "        adjacency_matrix = adjacency_matrix[:64, :64]  # 일부 노드만 사용하도록 조정\n",
    "        x = torch.matmul(adjacency_matrix, x.t()).t()  # x를 변환하여 행렬 곱셈이 가능하게 함\n",
    "        x = torch.relu(x)\n",
    "        x = torch.matmul(adjacency_matrix, x.t()).t()  # x를 다시 변환하여 행렬 곱셈이 가능하게 함\n",
    "        x = self.gc2(x)\n",
    "        return x\n",
    "    \n",
    "train_x, train_adjacency, train_y = create_graph_data(trainDF, features, target)\n",
    "test_x, test_adjacency, test_y = create_graph_data(testDF, features, target)\n",
    "\n",
    "input_dim = len(features)\n",
    "hidden_dim = 64\n",
    "output_dim = 1  # 예측할 값이 하나이므로 output_dim을 1로 설정\n",
    "model = GCN(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 학습\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_x, train_adjacency)\n",
    "    loss = criterion(output, train_y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# 테스트\n",
    "with torch.no_grad():\n",
    "    test_output = model(test_x, test_adjacency)\n",
    "    test_loss = criterion(test_output, test_y)\n",
    "    print(f'Test Loss: {test_loss.item()}')\n",
    "    \n",
    "evaluate_regression_model(model, testDF, criterion, threshold=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f33bcb6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fold_losses = []\n",
    "fold_accuracies = []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(train_x_tensor)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    \n",
    "    fold_train_x, fold_test_x = train_x_tensor[train_index], train_x_tensor[test_index]\n",
    "    fold_train_y, fold_test_y = train_y_tensor[train_index] / 60, train_y_tensor[test_index] / 60\n",
    "\n",
    "    # Move fold data to PyTorch tensors and then to the device\n",
    "    fold_train_x = fold_train_x.to(device)\n",
    "    fold_train_y = fold_train_y.to(device)\n",
    "    fold_test_x = fold_test_x.to(device)\n",
    "    fold_test_y = fold_test_y.to(device)\n",
    "\n",
    "    # Create DataLoader for training and testing data\n",
    "    train_dataset = TensorDataset(fold_train_x, fold_train_y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    test_dataset = TensorDataset(fold_test_x, fold_test_y)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # Specify the number of time steps and features\n",
    "    timesteps = 10\n",
    "    feature_num = len(features)\n",
    "\n",
    "    # Create an instance of the AirlineDelayPredictionModel class\n",
    "    model = AirlineDelayPredictionModel(timesteps, feature_num).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=0.2)\n",
    "    # Train the model\n",
    "    train_model(model, train_loader, criterion, optimizer, scheduler, epochs=20)\n",
    "    \n",
    "    # Evaluate the model on the test data\n",
    "    fold_loss, fold_accuracy = evaluate_regression_model(model, test_loader, criterion, threshold=0.1)\n",
    "    print(f\"Fold {fold + 1} - Test Loss: {fold_loss}, Accuracy: {fold_accuracy}\")\n",
    "\n",
    "    fold_losses.append(fold_loss)\n",
    "    fold_accuracies.append(fold_accuracy)\n",
    "\n",
    "average_loss = sum(fold_losses) / len(fold_losses)\n",
    "average_accuracy = sum(fold_accuracies) / len(fold_accuracies)\n",
    "\n",
    "print(f\"Overall Performance - Average Test Loss: {average_loss}, Average Accuracy: {average_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534ff3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# 학습 데이터 스케일링\n",
    "train_x_scaled = scaler.fit_transform(train_x)\n",
    "\n",
    "# PyTorch 텐서로 변환\n",
    "train_x_tensor = torch.tensor(train_x_scaled, dtype=torch.float32)\n",
    "train_y_tensor = torch.tensor(train_y.values, dtype=torch.float32)\n",
    "\n",
    "# 테스트 데이터 스케일링\n",
    "test_x_scaled = scaler.fit_transform(test_x)\n",
    "\n",
    "# PyTorch 텐서로 변환\n",
    "test_x_tensor = torch.tensor(test_x_scaled, dtype=torch.float32)\n",
    "test_y_tensor = torch.tensor(test_y.values, dtype=torch.float32)\n",
    "\n",
    "#test_x_scaled = scaler.transform(test_x.values)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "#train_x_tensor, train_y_tensor = torch.tensor(train_x_scaled, dtype=torch.float32), torch.tensor(train_y, dtype=torch.float32)\n",
    "#test_x_tensor, test_y_tensor = torch.tensor(test_x_scaled, dtype=torch.float32), torch.tensor(test_y.values, dtype=torch.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48357939",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_dataset = TensorDataset(test_x, test_y)\n",
    "final_test_loader = DataLoader(final_test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Create a new instance of the model for the final test\n",
    "final_model = AirlineDelayPredictionModel(timesteps, features)\n",
    "\n",
    "# Train the final model on the entire training set\n",
    "final_optimizer = optim.Adam(final_model.parameters(), lr=0.001)\n",
    "final_train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "train_model(final_model, final_train_loader, criterion, final_optimizer, epochs=10)\n",
    "\n",
    "# Evaluate the final model on the entire test set\n",
    "final_test_loss, final_test_accuracy = evaluate_model(final_model, final_test_loader, criterion)\n",
    "print(f\"Final Test Loss: {final_test_loss}, Final Accuracy: {final_test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c7b29c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab489a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
